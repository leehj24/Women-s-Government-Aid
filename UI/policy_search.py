# -*- coding: utf-8 -*-
# policy_search.py â€” ì—¬ì„± ì •ì±… ê²€ìƒ‰/ì¶”ì²œ ì—”ì§„ (score ì œê±° + ì¿¼ë¦¬ ê¸°ë°˜ ë‚˜ì´í•„í„° í†µí•©)

import os, re, time, json, argparse, hashlib
from datetime import datetime, date
from typing import List, Optional, Literal, Tuple
from pathlib import Path

import numpy as np
import pandas as pd
from dateutil import parser
from sentence_transformers import SentenceTransformer
import faiss, torch

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê²½ë¡œ/íŒŒì¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€
HERE = Path(__file__).resolve().parent

CACHE_DIR = Path(os.getenv("POLICY_CACHE_DIR", ".policy_cache")).expanduser()
if not CACHE_DIR.is_absolute():
    CACHE_DIR = HERE / CACHE_DIR
CACHE_DIR.mkdir(parents=True, exist_ok=True)

DATA_ENV = os.getenv("POLICY_XLSX", "")
CANDIDATES = [
    Path(DATA_ENV).expanduser(),
    HERE / "ì—¬ì„±ë§ì¶¤ì •ì±…_ìš”ì•½_2ì°¨_ê²°ê³¼_ë³‘í•©.xlsx",
    HERE.parent / "ì—¬ì„±ë§ì¶¤ì •ì±…_ìš”ì•½_2ì°¨_ê²°ê³¼_ë³‘í•©.xlsx",
]

FILE_PATH: Optional[Path] = None
for p in CANDIDATES:
    if p and str(p).strip() and p.is_file():
        FILE_PATH = p
        break
if FILE_PATH is None:
    hits = list(HERE.rglob("ì—¬ì„±ë§ì¶¤ì •ì±…_ìš”ì•½_2ì°¨_ê²°ê³¼_ë³‘í•©.xlsx"))
    if hits:
        FILE_PATH = hits[0]
if FILE_PATH is None:
    tried = "\n  - " + "\n  - ".join(str(p) for p in CANDIDATES)
    raise FileNotFoundError(
        "[ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ]\në‹¤ìŒ ìœ„ì¹˜ë¥¼ í™•ì¸í•˜ì„¸ìš”:\n"
        f"{tried}\në˜ëŠ” í™˜ê²½ë³€ìˆ˜ POLICY_XLSXì— ì •í™•í•œ ê²½ë¡œë¥¼ ì„¤ì •í•˜ì„¸ìš”."
    )
FILE_PATH = FILE_PATH.resolve()
print(f"âœ… ë°ì´í„° íŒŒì¼: {FILE_PATH}")
print(f"âœ… ìºì‹œ ë””ë ‰í„°ë¦¬: {CACHE_DIR}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë°ì´í„° ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€
df_raw = pd.read_excel(FILE_PATH)
# detail ë¼ìš°íŒ…ìš© ì›ë³¸ ì¸ë±ìŠ¤ ë³´ì¡´
df_raw = df_raw.reset_index(drop=False).rename(columns={'index': 'orig_index'})
# ì œëª© ì—†ëŠ” í–‰ ì œê±°
df = df_raw.dropna(subset=["ì œëª©"]).copy()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì „ì²˜ë¦¬(ê²€ìƒ‰ë³¸ë¬¸) â”€â”€â”€â”€â”€â”€â”€â”€â”€
def normalize_text(text: str) -> str:
    if not isinstance(text, str):
        return ""
    t = text.replace("\r", " ").replace("\n", " ").replace("\t", " ")
    t = re.sub(r"(?m)^\s*([\-â€“â€”\u2010-\u2015\u2212\u2043\u2022\u25CB\u25CF\u25AA\u25A0\u30FB]|[0-9]+[.)])\s+", " ", t)
    t = re.sub(r"[\u2460-\u2473\u3251-\u325F\u32B1-\u32BF]", " ", t)
    t = re.sub(r"[â€»ï¼Š*]+", " ", t)
    t = re.sub(r"\s{2,}", " ", t).strip()
    return t

def clean_field_value(val: str, field_name: str) -> str:
    if not isinstance(val, str): return ""
    v = re.sub(rf"^\s*{re.escape(field_name)}\s*[:\-â€“â€”]?\s*", "", val.strip())
    return normalize_text(v)

def unify_text_natural(row: pd.Series) -> str:
    title   = normalize_text(row.get("ì œëª©",""))
    region  = normalize_text(row.get("ì§€ì—­",""))
    target  = clean_field_value(row.get("ì§€ì›ëŒ€ìƒ",""), "ì§€ì›ëŒ€ìƒ")
    content = clean_field_value(row.get("ì§€ì›ë‚´ìš©",""), "ì§€ì›ë‚´ìš©")
    parts = []
    if title:   parts.append(f"ì •ì±…ëª…ì€ {title}")
    if region:  parts.append(f"ì§€ì—­ì€ {region}")
    if target:  parts.append(f"ì§€ì›ëŒ€ìƒì€ {target}")
    if content: parts.append(f"ì§€ì›ë‚´ìš©ì€ {content}")
    if not parts: return ""
    if len(parts) == 1:  return parts[0] + "ì´ë‹¤."
    if len(parts) == 2:  return parts[0] + "ì´ê³ , " + parts[1] + "ì´ë‹¤."
    return "ì´ê³ , ".join(parts[:-1]) + "ì´ë©°, " + parts[-1] + "ì´ë‹¤."

TEXT_COL = "ê²€ìƒ‰ë³¸ë¬¸_nat"
df[TEXT_COL] = df.apply(unify_text_natural, axis=1)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì»¬ëŸ¼ ìë™íƒì§€ â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _norm_colname(s: str) -> str:
    return re.sub(r"[\s_/()\-Â·.,]+", "", s or "").lower()

def _find_col(df_in: pd.DataFrame, prefer: List[str], fuzzy: List[str]) -> Optional[str]:
    for c in prefer:
        if c in df_in.columns:
            return c
    nmap = {c: _norm_colname(c) for c in df_in.columns}
    want = [_norm_colname(x) for x in prefer+fuzzy]
    for c, nc in nmap.items():
        if any(w in nc for w in want):
            return c
    return None

CATEGORY_COL = _find_col(df, ['ì¹´í…Œê³ ë¦¬_ë¶„ë¥˜','category_label'], ['ì¹´í…Œê³ ë¦¬','category','label'])
SUPPORT_COL  = _find_col(df, ['ì§€ì›í˜•íƒœ_ë¶„ë¥˜','support_label'], ['ì§€ì›í˜•íƒœ','support','label'])

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì„ë² ë”©/FAISS â”€â”€â”€â”€â”€â”€â”€â”€â”€
MODEL_NAME = "nlpai-lab/KURE-v1"
device = (
    "cuda" if torch.cuda.is_available() else
    ("mps" if getattr(torch.backends, "mps", None) and torch.backends.mps.is_available() else "cpu")
)
print(f"âœ… ëª¨ë¸ ë¡œë“œ: {MODEL_NAME} (device={device})")
model = SentenceTransformer(MODEL_NAME, device=device)

def _corpus_digest(corpus: List[str]) -> str:
    h = hashlib.sha256()
    h.update(MODEL_NAME.encode("utf-8"))
    h.update(str(len(corpus)).encode("utf-8"))
    for line in corpus:
        if not isinstance(line, str):
            line = "" if pd.isna(line) else str(line)
        h.update(line.encode("utf-8", errors="ignore"))
        h.update(b"\n")
    return h.hexdigest()

def _safe_part(s: str) -> str:
    s = str(s)
    s_ascii = s.encode("ascii", "ignore").decode("ascii")
    s_ascii = re.sub(r"[^A-Za-z0-9._-]+", "_", s_ascii).strip("_.")
    return s_ascii or "corpus"

def _cache_paths(corpus: List[str]) -> Tuple[str, str, str]:
    base   = os.path.splitext(os.path.basename(str(FILE_PATH)))[0]
    digest = _corpus_digest(corpus)[:16]
    base_s  = _safe_part(base)
    model_s = _safe_part(MODEL_NAME.replace("/", "_"))
    key = f"{base_s}.{len(corpus)}.{model_s}.{digest}"
    if len(key) > 120:
        key = f"{base_s[:30]}.{len(corpus)}.{model_s[:30]}.{digest}"
    faiss_path = str(CACHE_DIR / f"{key}.faiss")
    npy_path   = str(CACHE_DIR / f"{key}.npy")
    meta_path  = str(CACHE_DIR / f"{key}.json")
    return faiss_path, npy_path, meta_path

def _save_cache(index: faiss.Index, embeddings: np.ndarray, meta_path: str, faiss_path: str, npy_path: str):
    faiss.write_index(index, faiss_path)
    np.save(npy_path, embeddings)
    meta = {
        "model": MODEL_NAME, "ntotal": int(index.ntotal),
        "embedding_dim": int(embeddings.shape[1]),
        "saved_at": datetime.now().isoformat(timespec="seconds"),
        "file": os.path.basename(str(FILE_PATH)), "text_col": TEXT_COL,
    }
    with open(meta_path, "w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)

def _load_cache(faiss_path: str, npy_path: str) -> Tuple[faiss.Index, np.ndarray]:
    index = faiss.read_index(faiss_path)
    embeddings = np.load(npy_path)
    return index, embeddings

def _build_or_load_index(corpus: List[str], force_rebuild: bool = False) -> Tuple[faiss.Index, np.ndarray]:
    faiss_path, npy_path, meta_path = _cache_paths(corpus)
    if (not force_rebuild) and os.path.exists(faiss_path) and os.path.exists(npy_path):
        try:
            index, embeddings = _load_cache(faiss_path, npy_path)
            if index.ntotal != len(corpus):
                raise RuntimeError("ìºì‹œ ntotal ë¶ˆì¼ì¹˜")
            print(f"âš¡ ìºì‹œ ë¡œë“œ ì™„ë£Œ: {faiss_path}")
            return index, embeddings
        except Exception as e:
            print(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨, ì¬ìƒì„±í•©ë‹ˆë‹¤. ì´ìœ : {e}")

    print("âœ… ì •ì±… ì„ë² ë”© ìƒì„±...")
    t0 = time.time()
    embeddings = model.encode(corpus, convert_to_numpy=True, batch_size=64, show_progress_bar=True).astype("float32")
    faiss.normalize_L2(embeddings)
    index = faiss.IndexFlatIP(embeddings.shape[1])
    index.add(embeddings)
    print(f"âœ… ì„ë² ë”© ì™„ë£Œ: {time.time()-t0:.2f}s, ë²¡í„°ìˆ˜={len(embeddings)}")

    try:
        _save_cache(index, embeddings, meta_path, faiss_path, npy_path)
        print(f"ğŸ’¾ ìºì‹œ ì €ì¥: {faiss_path}")
    except Exception as e:
        print(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨(ë¬´ì‹œ): {e}")

    return index, embeddings

FORCE_REBUILD = os.getenv("POLICY_REBUILD", "0") in ("1","true","True","YES","yes")
corpus = df[TEXT_COL].fillna("").tolist()
index, _embeddings = _build_or_load_index(corpus, force_rebuild=FORCE_REBUILD)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë‚˜ì´ í•„í„° (ì‹ ê·œ: ì¿¼ë¦¬/age_eff_ranges ì§€ì›) â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì¿¼ë¦¬ì—ì„œ ë‚˜ì´ êµ¬ê°„ ë½‘ê¸° (ì˜ˆ: "ë§Œ 20~29ì„¸", "20ëŒ€", "30ì„¸ ì´ìƒ/ë¯¸ë§Œ")
AGE_RANGE_RE  = re.compile(r"(?:ë§Œ\s*)?(\d{1,3})\s*[~\-]\s*(?:ë§Œ\s*)?(\d{1,3})\s*ì„¸")
AGE_SINGLE_RE = re.compile(r"(?:ë§Œ\s*)?(\d{1,3})\s*ì„¸\s*(ì´ìƒ|ì´ˆê³¼|ì´í•˜|ë¯¸ë§Œ)?")

def parse_query_age(text: str) -> List[Tuple[int,int]]:
    out: List[Tuple[int,int]] = []
    if not text:
        return out
    for a,b in AGE_RANGE_RE.findall(text):
        a,b = int(a), int(b)
        lo,hi = min(a,b), max(a,b)
        out.append((lo,hi))
    for n,b in AGE_SINGLE_RE.findall(text):
        n = int(n)
        if not b: out.append((n,n))
        elif b in ("ì´ìƒ","ì´ˆê³¼"): out.append((n + (b=="ì´ˆê³¼"), 200))
        elif b in ("ì´í•˜","ë¯¸ë§Œ"): out.append((0, n - (b=="ë¯¸ë§Œ")))
    m = re.search(r"(\d{1,2})\s*ëŒ€", text)
    if m:
        d = int(m.group(1)); out.append((d*10, d*10+9))
    return out

def _ranges_intersect(a: Tuple[float,float], b: Tuple[float,float]) -> bool:
    return max(a[0], b[0]) <= min(a[1], b[1])

def age_match_from_json(policy_ranges_str: Optional[str], req_ranges: List[Tuple[int,int]]) -> bool:
    """ì—‘ì…€ì— age_eff_ranges(JSON ë¬¸ìì—´)ê°€ ìˆì„ ë•Œ ì‚¬ìš©. ì—†ìœ¼ë©´ True."""
    try:
        pr = json.loads(policy_ranges_str) if policy_ranges_str else []
    except Exception:
        pr = []
    if not pr:
        pr = [(0,200)]
    if not req_ranges:
        return True
    for lo1,hi1 in pr:
        for lo2,hi2 in req_ranges:
            if _ranges_intersect((lo1,hi1),(lo2,hi2)):
                return True
    return False

# (ê¸°ì¡´) í…ìŠ¤íŠ¸ì—ì„œ ë‚˜ì´ ê·œì¹™ ì¶”ì¶œ
KW_RANGES = [
    (re.compile(r"ê°€ì„ê¸°\s*ì—¬ì„±|ê°€ì„ê¸°ì—¬ì„±"), (15,49)),
    (re.compile(r"ì²­ì†Œë…„"), (13,20)),
    (re.compile(r"ì•„ë™"), (0,12.999)),
    (re.compile(r"ì–´ë¦°ì´"), (3,13)),
    (re.compile(r"ë…¸ì¸|ê³ ë ¹ì|ì–´ë¥´ì‹ "), (65,float("inf"))),
]

def _norm_min(s):
    if not isinstance(s,str): return ""
    return re.sub(r"\s{2,}"," ", s.replace("\r"," ").replace("\n"," ").replace("\t"," ")).strip()

def extract_age_constraints(t):
    t = _norm_min(t or ""); cons=[]
    for m in re.finditer(r"(?:ë§Œ\s*)?(\d+)\s*ì„¸\s*ì´ìƒ\s*[~\-â€“]\s*(?:ë§Œ\s*)?(\d+)\s*ì„¸\s*ë¯¸ë§Œ", t):
        lo,hi=int(m[1]),int(m[2]); cons.append((lo,hi,True,False))
    for m in re.finditer(r"ë§Œ\s*(\d+)\s*ì„¸\s*ì´ìƒ\s*ë§Œ\s*(\d+)\s*ì„¸\s*ì´í•˜", t):
        lo,hi=int(m[1]),int(m[2]); cons.append((lo,hi,True,True))
    for m in re.finditer(r"(?:ë§Œ\s*)?(\d+)\s*ì„¸\s*[~\-â€“]\s*(?:ë§Œ\s*)?(\d+)\s*ì„¸", t):
        lo,hi=int(m[1]),int(m[2]); cons.append((lo,hi,True,True))
    for m in re.finditer(r"(?:ë§Œ\s*)?(\d+)\s*[~\-â€“]\s*(?:ë§Œ\s*)?(\d+)\s*ì„¸", t):
        lo,hi=int(m[1]),int(m[2]); cons.append((lo,hi,True,True))
    for m in re.finditer(r"(?:ë§Œ\s*)?(\d+)\s*ì„¸\s*(ì´ìƒ|ì´í•˜|ë¯¸ë§Œ|ì´ˆê³¼|ì„¸ì´ìƒ|ì„¸ì´í•˜|ì„¸ë¯¸ë§Œ)", t):
        v=int(m[1]); typ=m[2]
        if typ in ("ì´ìƒ","ì„¸ì´ìƒ"): cons.append((v,None,True,None))
        elif typ in ("ì´í•˜","ì„¸ì´í•˜"): cons.append((None,v,None,True))
        elif typ in ("ë¯¸ë§Œ","ì„¸ë¯¸ë§Œ"): cons.append((None,v,None,False))
        elif typ=="ì´ˆê³¼": cons.append((v,None,False,None))
    for m in re.finditer(r"ìƒí›„\s*(\d+)\s*ê°œì›”\s*[~\-â€“]\s*(?:ë§Œ\s*)?(\d+)\s*ì„¸", t):
        lo=float(m[1])/12.0; hi=int(m[2]); cons.append((lo,hi,True,True))
    return cons

def extract_kw_constraints(t):
    t=_norm_min(t or ""); out=[]
    for pat,(lo,hi) in KW_RANGES:
        if pat.search(t): out.append((float(lo), float(hi), True, True))
    return out

def _policy_text_matches_req_ranges(text: str, req_ranges: List[Tuple[int,int]]) -> bool:
    """ì—‘ì…€ì— age_eff_ranges ì—†ì„ ë•Œ, 'ì§€ì›ëŒ€ìƒ' í…ìŠ¤íŠ¸ì—ì„œ êµ¬ê°„ ì¶”ì¶œí•´ ë¹„êµ."""
    if not req_ranges:
        return True
    cons = extract_age_constraints(text)
    kwc  = extract_kw_constraints(text)
    # cons/kwc â†’ ëŒ€í‘œ êµ¬ê°„(lo, hi) ì§‘ì•½
    ranges = []
    if cons:
        los=[c[0] for c in cons if c[0] is not None]
        his=[c[1] for c in cons if c[1] is not None]
        lo = max(los) if los else 0
        hi = min(his) if his else 200
        ranges.append((float(lo), float(hi)))
    if kwc:
        for lo,hi,_,_ in kwc:
            ranges.append((float(lo), float(hi)))
    if not ranges:   # ì‹ í˜¸ ì—†ìœ¼ë©´ ì œí•œ ì—†ìŒ
        return True
    for plo,phi in ranges:
        for rlo,rhi in req_ranges:
            if _ranges_intersect((plo,phi),(rlo,rhi)): return True
    return False

# ìƒë…„ì›”ì¼ â†’ ë‚˜ì´(ë…„)
def parse_birthdate(s: str):
    if not s: return None
    for fmt in ("%Y-%m-%d","%Y.%m.%d","%Y/%m/%d"):
        try:
            return datetime.strptime(s.strip(), fmt).date()
        except:
            pass
    try:
        return parser.parse(s.strip()).date()
    except:
        return None

def calc_age_years_precise(birth, ref=None):
    if not birth: return None
    if ref is None: ref = date.today()
    return (ref - birth).days/365.2425

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë¼ë²¨(ì¹´í…Œê³ ë¦¬/ì§€ì›í˜•íƒœ) ë¶€ë¶„ì¼ì¹˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€
_norm_label_re = re.compile(r"[\s/()\[\]{}Â·.,\-]+")
def _norm_label(s: str) -> str:
    return _norm_label_re.sub("", s or "").lower()

def _expand_queries(queries: List[str]) -> List[str]:
    out = set()
    for q in queries or []:
        q = (str(q) if q is not None else "").strip()
        if not q: continue
        out.add(q)
        for tok in re.split(r"[\/,|]", q):
            tok = tok.strip()
            if tok: out.add(tok)
        no_paren_chars    = re.sub(r"[()]", "", q)          # ëŒ€í•™(ì›)ìƒâ†’ëŒ€í•™ì›ìƒ
        no_paren_content  = re.sub(r"\([^)]*\)", "", q)      # ëŒ€í•™(ì›)ìƒâ†’ëŒ€í•™ìƒ
        out.update([no_paren_chars, no_paren_content])
        def _nz(s): return _norm_label(s) if s else s
        out.update({_nz(q), _nz(no_paren_chars), _nz(no_paren_content)})
        SYN = {
            "ëŒ€í•™(ì›)ìƒ": ["ëŒ€í•™ìƒ", "ëŒ€í•™ì›ìƒ"],
            "í•œë¶€ëª¨": ["í•œë¶€ëª¨ê°€ì¡±", "í•œë¶€ëª¨ê°€ì •"],
            "ê³ ë ¹ì": ["ë…¸ì¸", "ì–´ë¥´ì‹ "],
            "1ì¸ê°€êµ¬": ["1ì¸ ê°€êµ¬", "ë…ê±°"],
            "ì„ì‹ /ì¶œì‚°/ìœ¡ì•„": ["ì„ì‹ ", "ì¶œì‚°", "ìœ¡ì•„", "ì„ì‹ ì¶œì‚°ìœ¡ì•„"],
        }
        if q in SYN:
            out.update(SYN[q]); out.update(_nz(x) for x in SYN[q])
    return [x for x in out if x]

def _contains_any_substring(cell: str, queries: List[str]) -> bool:
    s = str(cell) if cell is not None else ""
    ex = _expand_queries(queries)
    if any(q for q in ex):
        if any(q in s for q in ex):    # ì›ë¬¸ ë¶€ë¶„ì¼ì¹˜
            return True
    s_norm = _norm_label(s)
    return any((qn and (qn in s_norm)) for qn in ex)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1ì°¨ í•„í„° â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_stage1_mask(df_in: pd.DataFrame,
                      categories: Optional[List[str]]=None,
                      supports: Optional[List[str]]=None,
                      region: Optional[str]=None,
                      birthdate: Optional[str]=None,
                      kw_text: Optional[str]=None) -> np.ndarray:
    n = len(df_in)
    mask = np.ones(n, dtype=bool)

    # ì¹´í…Œê³ ë¦¬/ì§€ì›í˜•íƒœ
    if categories:
        col = _find_col(df_in, ['ì¹´í…Œê³ ë¦¬_ë¶„ë¥˜','category_label'], ['ì¹´í…Œê³ ë¦¬','category','label'])
        if col:
            mask &= df_in[col].apply(lambda x: _contains_any_substring(x, categories)).to_numpy()
    if supports:
        col = _find_col(df_in, ['ì§€ì›í˜•íƒœ_ë¶„ë¥˜','support_label'], ['ì§€ì›í˜•íƒœ','support','label'])
        if col:
            mask &= df_in[col].apply(lambda x: _contains_any_substring(x, supports)).to_numpy()

    # ì§€ì—­: "" ë˜ëŠ” "ì „êµ­"ì´ë©´ ë¯¸ì ìš©, ê·¸ ì™¸ startswith
    r = (region or "").strip()
    if r and r != "ì „êµ­" and "ì§€ì—­" in df_in.columns:
        mask &= df_in["ì§€ì—­"].astype(str).str.startswith(r, na=False).to_numpy()

    # ë‚˜ì´ í•„í„°: dob â†’ ê°œì¸ë‚˜ì´, ì—†ìœ¼ë©´ kw_textì—ì„œ êµ¬ê°„ íŒŒì‹±
    req_ranges: List[Tuple[int,int]] = []
    if birthdate:
        b = parse_birthdate(birthdate)
        if b:
            age = int(calc_age_years_precise(b) or 0)
            req_ranges = [(age, age)]
    if (not req_ranges) and kw_text:
        req_ranges = parse_query_age(kw_text)

    if req_ranges:
        if "age_eff_ranges" in df_in.columns:
            mask &= df_in["age_eff_ranges"].apply(lambda s: age_match_from_json(s, req_ranges)).to_numpy()
        elif "ì§€ì›ëŒ€ìƒ" in df_in.columns:
            mask &= df_in["ì§€ì›ëŒ€ìƒ"].apply(lambda t: _policy_text_matches_req_ranges(t, req_ranges)).to_numpy()
        # else: í•´ë‹¹ ì»¬ëŸ¼ ì—†ìœ¼ë©´ í•„í„° ìŠ¤í‚µ

    return mask

def filter_by_user_inputs(df_in: pd.DataFrame,
                          region: Optional[str],
                          dob: Optional[str],
                          categories: Optional[List[str]],
                          supports: Optional[List[str]],
                          kw_text: Optional[str]=None) -> pd.DataFrame:
    mask = build_stage1_mask(
        df_in, categories=categories, supports=supports,
        region=region, birthdate=dob, kw_text=kw_text
    )
    return df_in[mask].copy()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì‹œë§¨í‹± ê²€ìƒ‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _faiss_search(query: str, top_k: Optional[int] = None):
    total = int(index.ntotal)
    k = total if (top_k is None or top_k <= 0 or top_k > total) else int(top_k)
    q = model.encode([query], convert_to_numpy=True).astype("float32")
    faiss.normalize_L2(q)
    D, I = index.search(q, k=k)
    return D[0], I[0]

def semantic_search(query: str, top_k: Optional[int] = None,
                    out: Literal["dataframe","json","csv"]="dataframe"):
    if not query or not str(query).strip():
        return pd.DataFrame() if out == "dataframe" else "[]"
    _, I = _faiss_search(str(query).strip(), top_k)
    rows = df.iloc[I].reset_index(drop=True).copy()
    return _format_output(rows, out)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì¶”ì²œ(ì¿¼ë¦¬ ì—†ìŒ) â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _last_date_from_period(s: str):
    if not isinstance(s,str) or not s.strip():
        return pd.NaT
    s2 = s.replace("~","-").replace("â€“","-").replace("â€”","-")
    parts = re.findall(r"\d{4}[./-]\d{1,2}[./-]\d{1,2}", s2)
    try:
        return parser.parse(parts[-1]) if parts else pd.NaT
    except:
        return pd.NaT

def recommend(region: Optional[str]="ì „êµ­",
              dob: Optional[str]=None,
              categories: Optional[List[str]]=None,
              supports: Optional[List[str]]=None,
              out: Literal["dataframe","json","csv"]="dataframe"):
    filtered = filter_by_user_inputs(df, region, dob, categories or [], supports or [], kw_text=None)
    tmp = filtered.copy()
    if "ì‹ ì²­ê¸°ê°„" in tmp.columns:
        tmp["_end"] = tmp["ì‹ ì²­ê¸°ê°„"].apply(_last_date_from_period)
        tmp = tmp.sort_values(by=["_end","ì œëª©"], ascending=[True,True]).drop(columns=["_end"])
    return _format_output(tmp.reset_index(drop=True), out)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ í†µí•© ì§„ì…ì  â”€â”€â”€â”€â”€â”€â”€â”€â”€
def find_policies(input: str = "",
                  topk: Optional[int] = None,
                  region: Optional[str] = "ì „êµ­",
                  dob: Optional[str] = None,
                  categories: Optional[List[str]] = None,
                  supports: Optional[List[str]] = None,
                  out: Literal["dataframe","json","csv"]="dataframe"):
    """
    - input ë¹„ì—ˆìœ¼ë©´: ì»¬ëŸ¼ ê¸°ë°˜ í•„í„°ë§Œ ì ìš©(ì „ì²´ ê°€ëŠ¥)
    - input ìˆìœ¼ë©´: ì‹œë§¨í‹± ê²€ìƒ‰ ìˆœì„œ ìœ ì§€ + ì»¬ëŸ¼ í•„í„° êµì§‘í•© (ì ìˆ˜ ì»¬ëŸ¼ ì—†ìŒ)
    - ë‚˜ì´ í•„í„°: dobê°€ ìˆìœ¼ë©´ ê°œì¸ ë‚˜ì´ ê¸°ì¤€, ì—†ìœ¼ë©´ inputì—ì„œ ë‚˜ì´ êµ¬ê°„ íŒŒì‹±
    """
    if not input or not str(input).strip():
        return recommend(region=region, dob=dob, categories=categories, supports=supports, out=out)

    _, I = _faiss_search(str(input).strip(), topk)
    # ì¿¼ë¦¬ì—ì„œ ë‚˜ì´ êµ¬ê°„ ì¶”ì¶œí•´ì„œ í•„í„°ì— ì „ë‹¬
    mask = build_stage1_mask(df, categories or [], supports or [], region, dob, kw_text=input)
    kept = [i for i in I if mask[i]]
    if not kept:
        kept = list(I)

    pos = {i: p for p, i in enumerate(I)}
    kept_sorted = sorted(kept, key=lambda i: pos[i])
    rows = df.iloc[kept_sorted].reset_index(drop=True).copy()
    return _format_output(rows, out)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì¶œë ¥ í¬ë§· â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _format_output(df_out: pd.DataFrame, out: Literal["dataframe","json","csv"]):
    if out == "dataframe":
        return df_out
    if out == "json":
        cols = [c for c in [
            "orig_index","ì œëª©","ì§€ì—­",
            "ì¹´í…Œê³ ë¦¬_ë¶„ë¥˜","category_label",
            "ì§€ì›í˜•íƒœ_ë¶„ë¥˜","support_label",
            "ì§€ì›í˜•íƒœ","ì‹ ì²­ê¸°ê°„","ì‹ ì²­ë°©ë²•","ì ‘ìˆ˜ê¸°ê´€",
            "ì§€ì›ëŒ€ìƒ","ì§€ì›ë‚´ìš©","ë¬¸ì˜ì²˜","ê¸°íƒ€","detail_url"
        ] if c in df_out.columns]
        return json.dumps(df_out[cols].to_dict(orient="records"), ensure_ascii=False, indent=2)
    if out == "csv":
        return df_out.to_csv(index=False)
    raise ValueError("out must be one of {'dataframe','json','csv'}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Flaskì—ì„œ ì›ë³¸ DF â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_base_df() -> pd.DataFrame:
    return df_raw.copy()

# (ì˜µì…˜) CLI
def _parse_list(s: Optional[str]) -> List[str]:
    if not s: return []
    return [x.strip() for x in re.split(r"[;,/|,]", s) if x.strip()]

def main(argv: Optional[List[str]] = None):
    import sys
    ap = argparse.ArgumentParser(description="ì—¬ì„± ì •ì±… ê²€ìƒ‰/ì¶”ì²œ ì—”ì§„")
    sub = ap.add_subparsers(dest="cmd", required=True)

    ap_find = sub.add_parser("find")
    ap_find.add_argument("--input", type=str, default="")
    ap_find.add_argument("--topk", type=int, default=0)
    ap_find.add_argument("--region", type=str, default="ì „êµ­")
    ap_find.add_argument("--dob", type=str, default=None)
    ap_find.add_argument("--categories", type=str, default=None)
    ap_find.add_argument("--supports", type=str, default=None)
    ap_find.add_argument("--out", type=str, choices=["dataframe","json","csv"], default="json")
    ap_find.add_argument("--rebuild", action="store_true")

    if argv is None:
        argv = sys.argv[1:]
    args = ap.parse_args(argv)

    if getattr(args, "rebuild", False):
        global index, _embeddings
        print("ğŸ”„ ìºì‹œ ì¬ìƒì„±")
        index, _embeddings = _build_or_load_index(corpus, force_rebuild=True)

    res = find_policies(
        input=args.input, topk=(None if args.topk==0 else args.topk),
        region=args.region, dob=args.dob,
        categories=_parse_list(args.categories), supports=_parse_list(args.supports),
        out=args.out
    )
    if isinstance(res, pd.DataFrame):
        print(res.to_csv(index=False, sep="\t"))
    else:
        print(res)

if __name__ == "__main__":
    import sys
    in_ipy = ("ipykernel" in sys.modules) or ("IPython" in sys.modules)
    if not in_ipy:
        main()
